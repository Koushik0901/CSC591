# Scaling LLMs

_Practical challenges with scaling out the training of LLMs on multi-GPU configurations to GPT-3-quality models_

## Key Objectives

 1. To be able to implement a CUDA program that uses more than one GPU
 2. To be able to describe the relationship between ML model design and computational requirements
 3. To be able to summarise the technical details of Narayanan et al. (2021)

## Key Deliverables

|Name  |Date        |Description|
|------|------------|-----------|
|Report|18 Aug 2024|A report describing a CUDA implementation that uses multiple GPUs|
|Presentation|18 Aug 2024|A 20-minute presentation of the term project to a research audience|

### Report

The report should be in .pdf format and not longer than six pages. It is recommended, but not required, to use [the standard format of the Association of Computing Machinery (ACM)](https://www.acm.org/publications/proceedings-template), preferably in double-column layout.
The focus of the report should be on describing how you worked through these milestones. 
The grade will be the percentage of milestones achieved.

In preparing the report, you should work through the following milestones:

 - TBD. Probably matrix multiplication is complex enough for learning?

### Presentation

TBD. A summary of the term to an audience that includes the AMPS research lab of Dr. Chester and other CSC 591 students supervised by Dr. Chester in the Summer 2024 semester.

## Tools & Resources

The following tools and resources might help get the project off the ground faster:
  * TBD

